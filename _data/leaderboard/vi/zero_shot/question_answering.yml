MLQA:
  GPT-3.5:
    EM: 0.0
    EM_std: 0.0
    F1: 0.25
    F1_std: 0.0
  GPT-4:
    EM: 0.0
    EM_std: 0.0
    F1: 0.27
    F1_std: 0.0
  LLaMa-2 13B:
    EM: 0.0
    EM_std: 0.02
    F1: 0.05
    F1_std: 0.0
  LLaMa-2 7B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.06
    F1_std: 0.0
  URA-LLaMa 13B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.15
    F1_std: 0.0
  URA-LLaMa 70B:
    EM: 0.04
    EM_std: 0.0
    F1: 0.28
    F1_std: 0.0
  URA-LLaMa 7B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.16
    F1_std: 0.0
  Vietcuna 7B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.0
    F1_std: 0.0
XQuAD:
  GPT-3.5:
    EM: 0.0
    EM_std: 0.0
    F1: 0.24
    F1_std: 0.0
  GPT-4:
    EM: 0.0
    EM_std: 0.0
    F1: 0.27
    F1_std: 0.0
  LLaMa-2 13B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.04
    F1_std: 0.0
  LLaMa-2 7B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.05
    F1_std: 0.0
  URA-LLaMa 13B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.14
    F1_std: 0.0
  URA-LLaMa 70B:
    EM: 0.06
    EM_std: 0.0
    F1: 0.3
    F1_std: 0.0
  URA-LLaMa 7B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.14
    F1_std: 0.0
  Vietcuna 7B:
    EM: 0.0
    EM_std: 0.0
    F1: 0.0
    F1_std: 0.0
mlqa:
  llama3:
    EM: 0.5
    EM_std: 0.0
    F1: 0.7092436974789916
    F1_std: 0.013235294117647123
